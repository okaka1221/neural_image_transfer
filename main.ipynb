{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "import time\n",
    "import IPython.display as display\n",
    "import PIL.Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像を読み込み配列に変換し正規化する\n",
    "def load_image(input_path, size):\n",
    "    image = tf.keras.preprocessing.image.load_img(input_path, target_size=size)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image /= 255\n",
    "    return image\n",
    "\n",
    "\n",
    "# テンソルを画像に戻す\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.numpy()\n",
    "    tensor *= 255\n",
    "    tensor = np.array(tensor, dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        assert tensor.shape[0] == 1\n",
    "        tensor = tensor[0]\n",
    "    return PIL.Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleContentModel():\n",
    "    def __init__(self):\n",
    "        # VGG19のどの層の出力を使うか指定する\n",
    "        self.style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "        self.content_layers = ['block5_conv2']\n",
    "        \n",
    "        self.num_style_layers = len(self.style_layers)        \n",
    "        self.vgg = self.get_vgg_model()\n",
    "        self.vgg.trainable = False\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs * 255)\n",
    "        vgg_outputs = self.vgg(preprocessed_input)\n",
    "        \n",
    "        style_outputs, content_outputs = (vgg_outputs[:self.num_style_layers], vgg_outputs[self.num_style_layers:])\n",
    "        style_outputs = [self.gram_matrix(style_output) for style_output in style_outputs]\n",
    "        \n",
    "        style_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)}\n",
    "        content_dict = {content_name:value  for content_name, value in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "        return {'style':style_dict, 'content':content_dict}\n",
    "    \n",
    "    # Keras API を利用してVGG19を取得する  \n",
    "    def get_vgg_model(self):\n",
    "        vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "        vgg.trainable = False\n",
    "\n",
    "        outputs = [vgg.get_layer(name).output for name in (self.style_layers + self.content_layers)]\n",
    "        model = tf.keras.Model(vgg.input, outputs)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # グラム行列を計算する\n",
    "    def gram_matrix(self, input_tensor):\n",
    "        result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "        input_shape = tf.shape(input_tensor)\n",
    "        num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
    "        return result / num_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合計損失を計算する\n",
    "def compute_loss(model, base_image, style_targets, content_targets, style_weight, content_weight):\n",
    "    model_outputs = model(base_image)\n",
    "    style_outputs = model_outputs['style']\n",
    "    content_outputs = model_outputs['content']\n",
    "    \n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / len(style_outputs)\n",
    "    \n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / len(content_outputs)\n",
    "    \n",
    "    loss = style_loss + content_loss\n",
    "    return loss, style_loss, content_loss\n",
    "\n",
    "# 損失を元に勾配を計算する\n",
    "@tf.function()\n",
    "def compute_grads(params):\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_loss = compute_loss(**params)\n",
    "        \n",
    "    grads = tape.gradient(all_loss[0], params['base_image'])\n",
    "    return grads, all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(style_path, content_path, num_iteration, style_weight, content_weight, display_interval):\n",
    "    size = image.load_img(content_path).size[::-1]\n",
    "    noise_image = np.random.uniform(-20, 20, (1, size[0], size[1], 3)).astype(np.float32) / 255\n",
    "    content_image = load_image(content_path, size)\n",
    "    style_image = load_image(style_path, size)\n",
    "    \n",
    "    model = StyleContentModel()\n",
    "    style_targets = model(style_image)['style']\n",
    "    content_targets = model(content_image)['content']\n",
    "    \n",
    "    # 生成画像のベースとしてノイズ画像を使う\n",
    "    # ベースにはコンテンツ画像またはスタイル画像を用いることもできる\n",
    "    base_image = tf.Variable(noise_image)\n",
    "    \n",
    "    opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "    \n",
    "    params = {\n",
    "        'model': model,\n",
    "        'base_image': base_image,\n",
    "        'style_targets': style_targets,\n",
    "        'content_targets': content_targets,\n",
    "        'style_weight': style_weight,\n",
    "        'content_weight': content_weight\n",
    "    }\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_image = None\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(num_iteration):\n",
    "        grads, all_loss = compute_grads(params)\n",
    "        loss, style_loss, content_loss = all_loss\n",
    "        \n",
    "        opt.apply_gradients([(grads, base_image)])\n",
    "        clipped_image = tf.clip_by_value(base_image, clip_value_min=0., clip_value_max=255.0)\n",
    "        base_image.assign(clipped_image)\n",
    "        \n",
    "        # 損失が減らなくなったら最適化を終了する        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_image = base_image\n",
    "        elif loss > best_loss:\n",
    "            tensor_to_image(base_image).save('output_' + str(i+1) + '.jpg')\n",
    "            break\n",
    "            \n",
    "        if (i + 1) % display_interval == 0:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(tensor_to_image(base_image))\n",
    "            tensor_to_image(base_image).save('output_' + str(i+1) + '.jpg')\n",
    "            print(f'Train step: {i+1}')\n",
    "            print('Total loss: {:.4e}, Style loss: {:.4e}, Content loss: {:.4e}'.format(loss, style_loss, content_loss))\n",
    "            \n",
    "    print('Total time: {:.4f}s'.format(time.time() - start))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(tensor_to_image(base_image))\n",
    "    \n",
    "    return best_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_path = '../input/neural-image-transfer/StarryNight.jpg'\n",
    "content_path = '../input/neural-image-transfer/FlindersStStation.jpg'\n",
    "num_iteration = 5000\n",
    "style_weight = 1e-2\n",
    "content_weight = 1e3\n",
    "display_interval = 100\n",
    "\n",
    "best_image = run_style_transfer(style_path, content_path, num_iteration, style_weight, content_weight, display_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
